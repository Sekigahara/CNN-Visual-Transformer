{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cab8b6-2cdf-4d84-8d9f-af0d9c21c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load cifar dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7fb395-5d40-4ec7-9db4-7bae238cacd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx_train\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714cc01c-7ce2-45e5-8c3f-b394dd5f782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195cb8f-d2ba-4d70-a0ce-6d5430295f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "y_train_categorical = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test_categorical = tf.keras.utils.to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee8832-a9c0-4da5-9869-596104d4316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "input_shape_vgg = (256, 256, 3)\n",
    "\n",
    "# Define Hyperparameter\n",
    "# ViT Base\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 10  # For real training, use num_epochs=100. 10 is a test value\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 12\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 12\n",
    "mlp_head_units = [\n",
    "    3072,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f09301-6c6f-4743-b5d9-f60a629678b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.vision_transformers.patches import Patches\n",
    "from modules.vision_transformers.patch_encoder import PatchEncoder, mlp\n",
    "\n",
    "# Data Augmentation for resize\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.Normalization(),\n",
    "    tf.keras.layers.Resizing(image_size, image_size)\n",
    "])\n",
    "\n",
    "# Compute the average and variance of the training data for normalization purpose\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afdea7e-c99b-4d1d-a35f-b36926b276a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def create_classifier():\n",
    "    # Inputs for CNN\n",
    "    inputs_cnn = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    vgg = tf.keras.applications.VGG16(\n",
    "        include_top=False,\n",
    "        input_shape=input_shape_vgg\n",
    "        weights='imagenet',\n",
    "        classifier_activation=None\n",
    "    )\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(128, kernel_size=(3, 3), strides=(2, 2), activation='relu', padding='same')(inputs_cnn)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=(2, 2), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=(2, 2), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    outputs_cnn = tf.keras.layers.Flatten()(x)\n",
    "    \n",
    "    # Input for Transformer\n",
    "    inputs_transformers = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Apply augment resize\n",
    "    augmented = data_augmentation(inputs_transformers)\n",
    "    \n",
    "    # Create Patches\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "\n",
    "    # Encode every patches position\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    for _ in range(transformer_layers):\n",
    "        # Create multilayers about how many transformers layer needed\n",
    "        x1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    \n",
    "        # Create a multi-head attention layer\n",
    "        attention_output = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection 1\n",
    "        x2 = tf.keras.layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # layer normalization 2\n",
    "        x3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "        # NLP\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "\n",
    "        # Skip Connection\n",
    "        encoded_patches = tf.keras.layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = tf.keras.layers.Flatten()(representation)\n",
    "    representation_transformers = tf.keras.layers.Dropout(0.3)(representation)\n",
    "\n",
    "    # Concat\n",
    "    concatenated = tf.keras.layers.Concatenate()([outputs_cnn, representation_transformers])\n",
    "    \n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.3)\n",
    "    \n",
    "    # Classify outputs.\n",
    "    logits = tf.keras.layers.Dense(num_classes)(concatenated)\n",
    "    # Create the Keras model.\n",
    "    model = tf.keras.Model(inputs=[inputs_cnn, inputs_transformers], outputs=logits)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a251b7-e0dd-4ad9-9216-6226c9302f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d2e681-150e-4de2-8dda-0917d5473070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.TopKCategoricalAccuracy(5, name='top-5-accuracy')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ca1f2-770f-4a00-8f61-c9c5c158e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=[x_train, x_train],\n",
    "    y=y_train_categorical,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=([x_test, x_test], y_test_categorical)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
